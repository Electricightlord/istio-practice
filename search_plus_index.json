{"./":{"url":"./","title":"Introduction","keywords":"","body":"Istio深度解析与项目实践 Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 12:40:49 "},"content/istio-introduction/":{"url":"content/istio-introduction/","title":"微服务架构中的基础设施","keywords":"","body":"作为一种架构模式，微服务将复杂系统切分为数十乃至上百个小服务，每个服务负责实现一个独立的业务逻辑。这些小服务易于被小型的软件工程师团队所理解和修改，并带>来了语言和框架选择灵活性，缩短应用开发上线时间，可根据不同的工作负载和资源要求对服务进行独立缩扩容等优势。 另一方面，当应用被拆分为多个微服务进程后，进程内的方法调用变成了了进程间的远程调用。引入了对大量服务的连接、管理和监控的复杂性。 该变化带来了分布式系统的一系列问题，例如： 如何找到服务的提供方？ 如何保证远程方法调用的可靠性？ 如何保证服务调用的安全性？ 如何降低服务调用的延迟？ 如何进行端到端的调试？ 另外生产部署中的微服务实例也增加了运维的难度,例如： 如何收集大量微服务的性能指标已进行分析？ 如何在不影响上线业务的情况下对微服务进行升级？ 如何测试一个微服务集群部署的容错和稳定性？ 这些问题涉及到成百上千个服务的通信、管理、部署、版本、安全、故障转移、策略执行、遥测和监控等，要解决这些微服务架构引入的问题并非易事。 让我们来回顾一下微服务架构的发展过程。在出现服务网格之前，我们最开始在微服务应用程序内理服务之间的通讯逻辑，包括服务发现，熔断，重试，超时，加密，限流等>逻辑。 在一个分布式系统中，这部分逻辑比较复杂，为了为微服务应用提供一个稳定、可靠的基础设施层，避免大家重复造轮子，并减少犯错的可能，一般会通过对这部分负责服务>通讯的逻辑进行抽象和归纳，形成一个代码库供各个微服务应用程序使用，如下图所示： 公共的代码库减少了应用程序的开发和维护工作量，降低了由应用开发人员单独实现微服务通讯逻辑出现错误的机率，但还是存在下述问题： 公共的代码库减少了应用程序的开发和维护工作量，降低了由应用开发人员单独实现微服务通讯逻辑出现错误的机率，但还是存在下述问题： 微服务通讯逻辑对应用开发人员并不透明，应用开发人员需要理解并正确使用代码 库，不能将其全部精力聚焦于业务逻辑。 需要针对不同的语言/框架开发不同的代码库，反过来会影响微服务应用开发语言 和框架的选择，影响技术选择的灵活性。 随着时间的变化，代码库会存在不同的版本，不同版本代码库的兼容性和大量运行 环境中微服务的升级将成为一个难题。 可以将微服务之间的通讯基础设施层和TCP/IP协议栈进行类比。TCP/IP协议栈为操作系统中的所有应用提供基础通信服务，但TCP/IP协议栈和应用程序之间并没有紧密的耦合>关系，应用只需要使用TCP/IP协议提供的底层通讯功能,并不关心TCP/IP协议的实现，如IP如何进行路由，TCP如何创建链接等。 同样地，微服务应用也不应该需要关注服务发现，Load balancing，Retries，Circuit Breaker等微服务之间通信的底层细节。如果将为微服务提供通信服务的这部分逻辑从>应用程序进程中抽取出来，作为一个单独的进程进行部署，并将其作为服务间的通信代理，可以得到如下图所示的架构： 因为通讯代理进程伴随应用进程一起部署，因此形象地把这种部署方式称为“sidecar”/边车（即三轮摩托的挎斗）。 应用间的所有流量都需要经过代理，由于代理以sidecar方式和应用部署在同一台主机上，应用和代理之间的通讯可以被认为是可靠的。由代理来负责找到目的服务并负责通讯 的可靠性和安全等问题。 当服务大量部署时，随着服务部署的sidecar代理之间的连接形成了一个如下图所示的网格，该网格成为了微服务的通讯基础设施层，承载了微服务之间的所有流量，被称之为 Service Mesh（服务网格）。 _服务网格是一个基础设施层，用于处理服务间通信。云原生应用有着复杂的服务拓扑，服务网格保证请求可以在这些拓扑中可靠地穿梭。在实际应用当中，服务网格通常是由 一系列轻量级的网络代理组成的，它们与应用程序部署在一起，但应用程序不需要知道它们的存在。 William Morgan WHAT’S A SERVICE MESH? AND WHY DO I NEED ONE? _ 服务网格中有数量众多的Sidecar代理，如果对每个代理分别进行设置，工作量将非常巨大。为了更方便地对服务网格中的代理进行统一集中控制，在服务网格上增加了控制面 组件。 这里我们可以类比SDN的概念，控制面就类似于SDN网管中的控制器，负责路由策略的指定和路由规则下发；数据面类似于SDN网络中交换机，负责数据包的转发。 由于微服务的所有通讯都由服务网格基础设施层提供，通过控制面板和数据面板的配合，可以对这些通讯进行监控、托管和控制，以实现微服务灰度发布，调用分布式追踪，>故障注入模拟测试，动态路由规则，微服务闭环控制等管控功能。 Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 12:46:28 "},"content/istio-introduction/微服务架构的演进.html":{"url":"content/istio-introduction/微服务架构的演进.html","title":"微服务架构的演进","keywords":"","body":"作为一种架构模式，微服务将复杂系统切分为数十乃至上百个小服务，每个服务负责实现一个独立的业务逻辑。这些小服务易于被小型的软件工程师团队所理解和修改，并带 来了语言和框架选择灵活性，缩短应用开发上线时间，可根据不同的工作负载和资源要求对服务进行独立缩扩容等优势。 另一方面，当应用被拆分为多个微服务进程后，进程内的方法调用变成了了进程间的远程调用。引入了对大量服务的连接、管理和监控的复杂性。 该变化带来了分布式系统的一系列问题，例如： 如何找到服务的提供方？ 如何保证远程方法调用的可靠性？ 如何保证服务调用的安全性？ 如何降低服务调用的延迟？ 如何进行端到端的调试？ 另外生产部署中的微服务实例也增加了运维的难度,例如： 如何收集大量微服务的性能指标已进行分析？ 如何在不影响上线业务的情况下对微服务进行升级？ 如何测试一个微服务集群部署的容错和稳定性？ 这些问题涉及到成百上千个服务的通信、管理、部署、版本、安全、故障转移、策略执行、遥测和监控等，要解决这些微服务架构引入的问题并非易事。 让我们来回顾一下微服务架构的发展过程。在出现服务网格之前，我们最开始在微服务应用程序内理服务之间的通讯逻辑，包括服务发现，熔断，重试，超时，加密，限流等逻辑。 在一个分布式系统中，这部分逻辑比较复杂，为了为微服务应用提供一个稳定、可靠的基础设施层，避免大家重复造轮子，并减少犯错的可能，一般会通过对这部分负责服务通讯的逻辑进行抽象和归纳，形成一个代码库供各个微服务应用程序使用，如下图所示： 公共的代码库减少了应用程序的开发和维护工作量，降低了由应用开发人员单独实现微服务通讯逻辑出现错误的机率，但还是存在下述问题： 公共的代码库减少了应用程序的开发和维护工作量，降低了由应用开发人员单独实现微服务通讯逻辑出现错误的机率，但还是存在下述问题： 微服务通讯逻辑对应用开发人员并不透明，应用开发人员需要理解并正确使用代码 库，不能将其全部精力聚焦于业务逻辑。 需要针对不同的语言/框架开发不同的代码库，反过来会影响微服务应用开发语言 和框架的选择，影响技术选择的灵活性。 随着时间的变化，代码库会存在不同的版本，不同版本代码库的兼容性和大量运行 环境中微服务的升级将成为一个难题。 可以将微服务之间的通讯基础设施层和TCP/IP协议栈进行类比。TCP/IP协议栈为操作系统中的所有应用提供基础通信服务，但TCP/IP协议栈和应用程序之间并没有紧密的耦合关系，应用只需要使用TCP/IP协议提供的底层通讯功能,并不关心TCP/IP协议的实现，如IP如何进行路由，TCP如何创建链接等。 同样地，微服务应用也不应该需要关注服务发现，Load balancing，Retries，Circuit Breaker等微服务之间通信的底层细节。如果将为微服务提供通信服务的这部分逻辑从应用程序进程中抽取出来，作为一个单独的进程进行部署，并将其作为服务间的通信代理，可以得到如下图所示的架构： 因为通讯代理进程伴随应用进程一起部署，因此形象地把这种部署方式称为“sidecar”/边车（即三轮摩托的挎斗）。 应用间的所有流量都需要经过代理，由于代理以sidecar方式和应用部署在同一台主机上，应用和代理之间的通讯可以被认为是可靠的。由代理来负责找到目的服务并负责通讯的可靠性和安全等问题。 当服务大量部署时，随着服务部署的sidecar代理之间的连接形成了一个如下图所示的网格，该网格成为了微服务的通讯基础设施层，承载了微服务之间的所有流量，被称之为Service Mesh（服务网格）。 _服务网格是一个基础设施层，用于处理服务间通信。云原生应用有着复杂的服务拓扑，服务网格保证请求可以在这些拓扑中可靠地穿梭。在实际应用当中，服务网格通常是由 一系列轻量级的网络代理组成的，它们与应用程序部署在一起，但应用程序不需要知道它们的存在。 William Morgan WHAT’S A SERVICE MESH? AND WHY DO I NEED ONE? _ 服务网格中有数量众多的Sidecar代理，如果对每个代理分别进行设置，工作量将非常巨大。为了更方便地对服务网格中的代理进行统一集中控制，在服务网格上增加了控制面 组件。 这里我们可以类比SDN的概念，控制面就类似于SDN网管中的控制器，负责路由策略的指定和路由规则下发；数据面类似于SDN网络中交换机，负责数据包的转发。 由于微服务的所有通讯都由服务网格基础设施层提供，通过控制面板和数据面板的配合，可以对这些通讯进行监控、托管和控制，以实现微服务灰度发布，调用分布式追踪，故障注入模拟测试，动态路由规则，微服务闭环控制等管控功能。 Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 12:40:49 "},"content/istio-introduction/Istio服务网格.html":{"url":"content/istio-introduction/Istio服务网格.html","title":"Istio服务网格","keywords":"","body":"Istio是一个Service Mesh开源项目，是Google继Kubernetes之后的又一力作，主要参与的公司包括Google，IBM和Lyft。 凭借kubernetes良好的架构设计及其强大的扩展性，Google围绕kubernetes打造一个生态系统。Kubernetes用于微服务的编排（编排是英文Orchestration的直译，用大白话说就是描述一组微服务之间的关联关系，并负责微服务的部署、终止、升级、缩扩容等）。其向下用CNI(容器网络接口），CRI（容器运行时接口）标准接口可以对接不同的网络和容器运行时实现，提供微服务运行的基础设施。向上则用Istio提供了微服务治理功能。 由下图可见，Istio补充了Kubernetes生态圈的重要一环，是Google的微服务版图里一个里程碑式的扩张。 Google借Istio的力量推动微服务治理的事实标准，对Google自身的产品Google Cloud有极其重大的意义。其他的云服务厂商，如Redhat，Pivotal，Nginx，Buoyant等看到大势所趋，也纷纷跟进，宣布自身产品和Istio进行集成，以避免自己被落下，丢失其中的市场机会。 可以预见不久的将来，对于云原生应用而言，采用kubernetes进行服务部署和集群管理，采用Istio处理服务通讯和治理，将成为微服务应用的标准配置。 Istio服务包括网格由数据面和控制面两部分。 数据面由一组智能代理（Envoy）组成，代理部署为边车，调解和控制微服务之间所有的网络通信。 控制面负责管理和配置代理来路由流量，以及在运行时执行策略。 Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 12:40:49 "},"content/istio-introduction/Istio控制面.html":{"url":"content/istio-introduction/Istio控制面.html","title":"Istio控制面","keywords":"","body":"Istio控制面板包括3个组件:Pilot, Mixer和Istio-Auth。 Pilot Pilot维护了网格中的服务的标准模型，这个标准模型是独立于各种底层平台的。Pilot通过适配器和各底层平台对接，以填充此标准模型。 例如Pilot中的Kubernetes适配器通过Kubernetes API服务器得到kubernetes中pod注册信息的更改，入口资源以及存储流量管理规则等信息，然后将该数据被翻译为标准模型提供给Pilot使用。通过适配器模式，Pilot还可以从Mesos, Cloud Foundry, Consul中获取服务信息，也可以开发适配器将其他提供服务发现的组件集成到Pilot中。 除此以外，Pilo还定义了一套和数据面通信的标准API，API提供的接口内容包括服务发现 、负载均衡池和路由表的动态更新。通过该标准API将控制面和数据面进行了解耦，简化了设计并提升了跨平台的可移植性。基于该标准API已经实现了多种Sidecar代理和Istio的集成，除Istio目前集成的Envoy外，还可以和Linkerd, Nginmesh等第三方通信代理进行集成，也可以基于该API自己编写Sidecar实现。 Pilot还定义了一套DSL（Domain Specific Language）语言，DSL语言提供了面向业务的高层抽象，可以被运维人员理解和使用。运维人员使用该DSL定义流量规则并下发到Pilot，这些规则被Pilot翻译成数据面的配置，再通过标准API分发到Envoy实例，可以在运行期对微服务的流量进行控制和调整。 Mixer 在微服务应用中，通常需要部署一些基础的后端公共服务以用于支撑业务功能。这些基础设施包括策略类如访问控制，配额管理；以及遥测报告如APM，日志等。微服务应用和这些后端支撑系统之间一般是直接集成的，这导致了应用和基础设置之间的紧密耦合，如果因为运维原因需要对基础设置进行升级或者改动，则需要修改各个微服务的应用代码，反之亦然。 为了解决该问题，Mixer在应用程序代码和基础架构后端之间引入了一个通用中间层。该中间层解耦了应用和后端基础设施，应用程序代码不再将应用程序代码与特定后端集成在一起，而是与Mixer进行相当简单的集成，然后Mixer负责与后端系统连接。 Mixer主要提供了三个核心功能： 前提条件检查。允许服务在响应来自服务消费者的传入请求之前验证一些前提条件。前提条件可以包括服务使用者是否被正确认证，是否在服务的白名单上，是否通过ACL检查等等。 配额管理。 使服务能够在分配和释放多个维度上的配额，配额这一简单的资源管理工具可以在服务消费者对有限资源发生争用时，提供相对公平的（竞争手段）。Rate Limiting就是配额的一个例子。 遥测报告。使服务能够上报日志和监控。在未来，它还将启用针对服务运营商以及服务消费者的跟踪和计费流。 Mixer的架构如图所示: 首先，Sidecar会从每一次请求中收集相关信息，如请求的路径，时间，源IP，目地服务，tracing头，日志等，并请这些属性上报给Mixer。Mixer和后端服务之间是通过适配器进行连接的，Mixer将Sidecar上报的内容通过适配器发送给后端服务。 由于Sidecar只和Mixer进行对接，和后端服务之间并没有耦合，因此使用Mixer适配器机制可以接入不同的后端服务，而不需要修改应用的代码，例如通过不同的Mixer适配器，可以把Metrics收集到Prometheus或者InfluxDB，甚至可以在不停止应用服务的情况下动态切换后台服务。 其次，Sidecar在进行每次请求处理时会通过Mixer进行策略判断，并根据Mixer返回的结果决定是否继续处理该次调用。通过该方式，Mixer将策略决策移出应用层，使运维人员可以在运行期对策略进行配置，动态控制应用的行为，提高了策略控制的灵活性。例如可以配置每个微服务应用的访问白名单，不同客户端的Rate limiting，等等。 逻辑上微服务之间的每一次请求调用都会经过两次Mixer的处理：调用前进行策略判断，调用后进行遥测数据收集。Istio采用了一些机制来避免Mixer的处理影响Envoy的转发效率。 从上图可以看到，Istio在Envoy中增加了一个Mixer Filter，该Filter和控制面的Mixer组件进行通讯，完成策略控制和遥测数据收集功能。Mixer Filter中保存有策略判断所需的数据缓存，因此大部分策略判断在Envoy中就处理了，不需要发送请求到Mixer。另外Envoy收集到的遥测数据会先保存在Envoy的缓存中，每隔一段时间再通过批量的方式上报到Mixer。 Auth Istio支持双向SSL认证（Mutual SSL Authentication）和基于角色的访问控制（RBAC），以提供端到端的安全解决方案。 认证 Istio提供了一个内部的CA(证书机构),该CA为每个服务颁发证书，提供服务间访问的双向SSL身份认证，并进行通信加密，其架构如下图所示： 其工作机制如下： 部署时： CA监听Kubernetes API Server, 为集群中的每一个Service Account创建一对密钥和证书，并发送给Kubernetes API Server。注意这里不是为每个服务生成一个证书，而是为每个Service Account生成一个证书。Service Account和kubernetes中部署的服务可以是一对多的关系。Service Account被保存在证书的SAN(Subject Alternative Name)字段中。 当Pod创建时，Kubernetes根据该Pod关联的Service Account将密钥和证书以Kubernetes Secrets资源的方式加载为Pod的Volume，以供Envoy使用。 Pilot生成数据面的配置，包括Envoy需使用的密钥和证书信息，以及哪个Service Account可以允许运行哪些服务，下发到Envoy。 备注：如果是虚机环境，则采用一个Node Agent生成密钥，向Istio CA申请证书，然后将证书传递给Envoy。 运行时： 服务客户端的出站请求被Envoy接管。 客户端的Envoy和服务端的Envoy开始双向SSL握手。在握手阶段，客户端Envoy会验证服务端Envoy证书中的Service Account有没有权限运行该请求的服务，如没有权限，则认为服务端不可信，不能创建链接。 当加密TSL链接创建好后，请求数据被发送到服务端的Envoy，然后被Envoy通过一个本地的TCP链接发送到服务中。 鉴权 Istio“基于角色的访问控制”（RBAC）提供了命名空间，服务，方法三个不同大小粒度的服务访问权限控制。其架构如下图所示： 管理人员可以定制访问控制的安全策略，这些安全策略保存在Istio Config Store中。 Istio RBAC Engine从Config Store中获取安全策略，根据安全策略对客户端发起的请求进行判断，并返回鉴权结果（允许或者禁止）。 Istio RBAC Engine目前被实现为一个Mixer Adapter，因此其可以从Mixer传递过来的上下文中获取到访问请求者的身份（Subject）和操作请求（Action），并通过Mixer对访问请求进行策略控制，允许或者禁止某一次请求。 Istio Policy中包含两个基本概念： ServiceRole，定义一个角色，并为该角色指定对网格中服务的访问权限。指定角色访问权限时可以在命名空间，服务，方法的不同粒度进行设置。 ServiceRoleBinding，将角色绑定到一个Subject，可以是一个用户，一组用户或者一个服务。 Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 12:40:49 "},"content/istio-introduction/Istio数据面.html":{"url":"content/istio-introduction/Istio数据面.html","title":"Istio数据面","keywords":"","body":"Istio数据面以“边车”(sidecar)的方式和微服务一起部署，为微服务提供安全、快速、可靠的服务间通讯。由于Istio的控制面和数据面以标准接口进行交互，因此数据可以有多种实现，Istio缺省使用了Envoy代理的扩展版本。 Envoy是以C ++开发的高性能代理，用于调解服务网格中所有服务的所有入站和出站流量。Envoy的许多内置功能被Istio发扬光大，例如动态服务发现，负载均衡，TLS加密，HTTP/2 & gRPC代理，熔断器，路由规则，故障注入和遥测等。 Istio数据面支持的特性如下： Outbound特性 Inbound特性 Service authentication（服务认证） Service authentication（服务认证） Load Balancing（负载均衡） Authorization（鉴权） Retry and circuit breaker（重试和断路器） Rate limits（请求限流） Fine-grained routing（细粒度的路由） Load shedding（负载控制） Telemetry（遥测） Telemetry（遥测） Request Tracing（分布式追踪） Request Tracing（分布式追踪） Fault Injection（故障注入） Fault Injection（故障注入） 备注：Outbound特性是指服务请求侧的Sidecar提供的功能特性，而Inbound特性是指服务提供侧Sidecar提供的功能特性。一些特性如遥测和分布式跟踪需要两侧的Sidecar都提供支持；而另一些特性则只需要在一侧提供，例如鉴权只需要在服务提供侧提供，重试只需要在请求侧提供。 Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 12:40:49 "},"content/istio-introduction/应用场景-分布式调用跟踪.html":{"url":"content/istio-introduction/应用场景-分布式调用跟踪.html","title":"应用场景-分布式调用跟踪","keywords":"","body":"在微服务架构中，业务的调用链非常复杂，一个来自用户的请求可能涉及到几十个服务的协同处理。因此需要一个跟踪系统来记录和分析同一次请求在整个调用链上的相关事件，从而帮助研发和运维人员分析系统瓶颈，快速定位异常和优化调用链路。 Istio通过在Envoy代理上收集调用相关数据，实现了对应用无侵入的分布式调用跟踪分析。 Istio实现分布式调用追踪的原理如下图所示: Envoy收集一个端到端调用中的各个分段的数据，并将这些调用追踪信息发送给Mixer，Mixer Adapter 将追踪信息发送给相应的服务后端进行处理。整个调用追踪信息的生成流程不需要应用程序介入，因此不需要将分布式跟踪相关代码注入到应用程序中。 注意：应用仍需要在进行出口调用时将收到的入口请求中tracing相关的header转发出去，传递给调用链中下一个边车进行处理。 Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 12:40:49 "},"content/istio-introduction/应用场景-度量收集.html":{"url":"content/istio-introduction/应用场景-度量收集.html","title":"应用场景-度量收集","keywords":"","body":"Istio 实现度量收集的原理如下图所示: Envoy收集指标相关的原始数据，如请求的服务，HTTP状态码，调用时延等，这些收集到的指标数据被送到Mixer，通过Mixer Adapters 将指标信息转换后发送到后端的监控系统中。由于Mixer使用了插件机制，后端监控系统可以根据需要在运行期进行动态切换。 Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 12:40:49 "},"content/istio-introduction/应用场景-灰度发布.html":{"url":"content/istio-introduction/应用场景-灰度发布.html","title":"应用场景-灰度发布","keywords":"","body":"当应用上线以后，运维面临的一大挑战是如何能够在不影响已上线业务的情况下进行升级。无论进行了多么完善的测试，都无法保证线下测试时发现所有潜在故障。在无法百分百避免版本升级故障的情况下，需要通过一种方式进行可控的版本发布，把故障影响控制在可以接受的范围内，并可以快速回退。 可以通过灰度发布（又名金丝雀发布）来实现业务从老版本到新版本的平滑过渡，并避免升级过程中出现的问题对用户造成的影响。 Istio通过高度的抽象和良好的设计采用一致的方式实现了灰度发布。在发布新版本后，运维人员可以通过定制路由规则将特定的流量（如具有指定特征的测试用户）导入新版本服务中以进行测试。通过渐进受控地向新版本导入生产流量，可以最小化升级中出现的故障对用户的影响。 采用Istio进行灰度发布的流程如下图所示： 首先，通过部署新版本的服务，并将通过路由规则将金丝雀用户的流量导入到新版本服务中 测试稳定后，使用路由规则将生产流量逐渐导入到新版本系统中，如按5%，10%，50%，80%逐渐导入。 如果新版本工作正常，则最后将所有流量导入到新版本服务中，并将老版本服务下线；如中间出现问题，则可以将流量重新导回老版本，在新版本中修复故障后采用该流程重新发布。 Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 12:40:49 "},"content/istio-introduction/应用场景-断路器.html":{"url":"content/istio-introduction/应用场景-断路器.html","title":"应用场景-断路器","keywords":"","body":"在微服务架构中，存在着许许多多的服务单元，若一个服务出现故障，就会因依赖关系形成故障蔓延，最终导致整个系统的瘫痪，这样的架构相较传统架构就更加的不稳定。为了解决这样的问题，因此产生了断路器模式。 断路器模式指，在某个服务发生故障时，断路器的故障监控向调用放返回一个及时的错误响应，而不是长时间的等待。这样就不会使得调用线程因调用故障被长时间占用，从而避免了故障在整个系统中的蔓延。 Istio 实现断路器的原理如下图所示: 管理员通过destination policy设置断路触发条件，断路时间等参数。例如设置服务B发生10次5XX错误后断路15分钟。则当服务B的某一实例满足断路条件后，就会被从LB池中移除15分钟。在这段时间内，Envoy将不再把客户端的请求转发到该服务实例。 Istio的断路器还支持配置最大链接数，最大待处理请求数，最大请求数，每链接最大请求数，重试次数等参数。当达到设置的最大请求数后，新发起的请求会被Envoy直接拒绝。 Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 12:40:49 "},"content/istio-introduction/应用场景-故障注入.html":{"url":"content/istio-introduction/应用场景-故障注入.html","title":"应用场景-故障注入","keywords":"","body":"对于一个大型微服务应用而言，系统的健壮性非常重要。在微服务系统中存在大量的服务实例，当部分服务实例出现问题时，微服务应用需要具有较高的容错性，通过重试，断路，自愈等手段保证系统能够继续对外正常提供服务。因此在应用发布到生产系统强需要对系统进行充分的健壮性测试。 对微服务应用进行健壮性测试的一个最大的困难是如何对系统故障进行模拟。在一个部署了成百上千微服务的测试环境中，如果想通过对应用，主机或者交换机进行设置来模拟微服务之间的通信故障是非常困难的。 Istio通过服务网格承载了微服务之间的通信流量，因此可以在网格中通过规则进行故障注入，模拟部分微服务出现故障的情况，对整个应用的健壮性进行测试。 故障注入的原理如下图所示： 测试人员通过Pilot向Envoy注入了一个规则，为发向服务MS-B的请求加入了指定时间的延迟。当客户端请求发向MSB-B时，Envoy会根据该规则为该请求加入时延，引起客户的请求超时。通过设置规则注入故障的方式，测试人员可以很方便地模拟微服务之间的各种通信故障，对微服务应用的健壮性进行较为完整的模拟测试。 Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 12:40:49 "},"content/ingress/":{"url":"content/ingress/","title":"如何为服务网格选择入口网关","keywords":"","body":"在启用了Istio服务网格的Kubernetes集群中，缺省情况下只能在集群内部访问网格中的服务，要如何才能从外部网络访问这些服务呢？ Kubernetes和Istio提供了NodePort，LoadBalancer，Kubernetes Ingress，Istio Gateway等多种外部流量入口的方式，面对这么多种方式，我们在产品部署中应该如何选择？ 本章节将对Kubernetes和Istio对外提供服务的各种方式进行详细介绍和对比分析，并根据分析结果提出一个可用于产品部署的解决方案。 说明：阅读本章节要求读者了解Kubernetes和Istio的基本概念，包括Pod、Service、NodePort、LoadBalancer、Ingress、Gateway、VirtualService等。如对这些概念不熟悉，可以在阅读过程中参考文后的相关链接。 Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 12:59:40 "},"content/ingress/cluster-ip.html":{"url":"content/ingress/cluster-ip.html","title":"内部通讯-ClusterIP","keywords":"","body":"内部服务间的通信 首先，我们来回顾一下Kubernetes集群内部各个服务之间相互访问的方法。 Cluster IP Kubernetes以Pod作为应用部署的最小单位。kubernetes会根据Pod的声明对其进行调度，包括创建、销毁、迁移、水平伸缩等，因此Pod 的IP地址不是固定的，不方便直接采用Pod IP对服务进行访问。 为解决该问题，Kubernetes提供了Service资源，Service对提供同一个服务的多个Pod进行聚合。一个Service提供一个虚拟的Cluster IP，后端对应一个或者多个提供服务的Pod。在集群中访问该Service时，采用Cluster IP即可，Kube-proxy负责将发送到Cluster IP的请求转发到后端的Pod上。 Kube-proxy是一个运行在每个节点上的go应用程序，支持三种工作模式： userspace 该模式下kube-proxy会为每一个Service创建一个监听端口。发向Cluster IP的请求被Iptables规则重定向到Kube-proxy监听的端口上，Kube-proxy根据LB算法选择一个提供服务的Pod并和其建立链接，以将请求转发到Pod上。 该模式下，Kube-proxy充当了一个四层Load balancer的角色。由于kube-proxy运行在userspace中，在进行转发处理时会增加两次内核和用户空间之间的数据拷贝，效率较另外两种模式低一些；好处是当后端的Pod不可用时，kube-proxy可以重试其他Pod。 图片 - Kube-proxy userspace模式 图片来自：Kubernetes官网文档 iptables 为了避免增加内核和用户空间的数据拷贝操作，提高转发效率，Kube-proxy提供了iptables模式。在该模式下，Kube-proxy为service后端的每个Pod创建对应的iptables规则，直接将发向Cluster IP的请求重定向到一个Pod IP。 该模式下Kube-proxy不承担四层代理的角色，只负责创建iptables规则。该模式的优点是较userspace模式效率更高，但不能提供灵活的LB策略，当后端Pod不可用时也无法进行重试。 图片 - Kube-proxy iptables模式 图片来自：Kubernetes官网文档 ipvs 该模式和iptables类似，kube-proxy监控Pod的变化并创建相应的ipvs rules。ipvs也是在kernel模式下通过netfilter实现的，但采用了hash table来存储规则，因此在规则较多的情况下，Ipvs相对iptables转发效率更高。除此以外，ipvs支持更多的LB算法。如果要设置kube-proxy为ipvs模式，必须在操作系统中安装IPVS内核模块。 Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 13:10:01 "},"content/ingress/istio-sidecar-proxy.html":{"url":"content/ingress/istio-sidecar-proxy.html","title":"内部通讯-Sidecar Proxy","keywords":"","body":"Istio Sidecar Proxy Cluster IP解决了服务之间相互访问的问题，但从上面Kube-proxy的三种模式可以看到，Cluster IP的方式只提供了服务发现和基本的LB功能。如果要为服务间的通信应用灵活的路由规则以及提供Metrics collection，distributed tracing等服务管控功能,就必须得依靠Istio提供的服务网格能力了。 在Kubernetes中部署Istio后，Istio通过iptables和Sidecar Proxy接管服务之间的通信，服务间的相互通信不再通过Kube-proxy，而是通过Istio的Sidecar Proxy进行。请求流程是这样的：Client发起的请求被iptables重定向到Sidecar Proxy，Sidecar Proxy根据从控制面获取的服务发现信息和路由规则，选择一个后端的Server Pod创建链接，代理并转发Client的请求。 Istio Sidecar Proxy和Kube-proxy的userspace模式的工作机制类似，都是通过在用户空间的一个代理来实现客户端请求的转发和后端多个Pod之间的负载均衡。两者的不同点是：Kube-Proxy工作在四层，而Sidecar Proxy则是一个七层代理，可以针对HTTP，GRPS等应用层的语义进行处理和转发，因此功能更为强大，可以配合控制面实现更为灵活的路由规则和服务管控功能。 Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 13:10:22 "},"content/debug-istio/":{"url":"content/debug-istio/","title":"Istio故障定位方法","keywords":"","body":"服务网格为微服务提供了一个服务通信的基础设施层，统一为上层的微服务提供了服务发现，负载均衡，重试，断路等基础通信功能，以及服务路由，灰度发布，chaos测试等高级管控功能。 服务网格的引入大大降低了个微服务应用的开发难度，让微服务应用开发人员不再需要花费大量时间用于保障底层通讯的正确性上，而是重点关注于产生用户价值的业务需求。 然而由于微服务架构的分布式架构带来的复杂度并未从系统中消失，而是从各个微服务应用中转移到了服务网格中。由服务网格对所有微服务应用的通讯进行统一控制，好处是可以保证整个系统中分布式通讯策略的一致性，并可以方便地进行集中管控。 除微服务之间分布式调用的复杂度之外，服务网格在底层通讯和微服务应用之间引入了新的抽象层，为系统引入了一些额外的复杂度。在此情况下，如果服务网格自身出现故障，将对上层的微服务应用带来灾难性的影响。 当系统中各微服务应用之间的通讯出现异常时，我们可以通过服务网格提供的分布式调用跟踪，故障注入，服务路由等手段快速进行分析和处理。但如果服务网格系统自身出现问题的话，我们如何才能快速进行分析处理呢？ Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 12:40:49 "},"content/debug-istio/服务网格中的请求转发流程.html":{"url":"content/debug-istio/服务网格中的请求转发流程.html","title":"请求转发流程","keywords":"","body":"微服务之间的流量经过服务网格接管后，在微服务之间引入了多个代理层，微服务之间的通信变得更为复杂了。下图分析了一个客户端请求的流量是如何在服务网格中进行路由的。 备注：由于Istio Ingress Gateway的功能缺少API管理功能，因此下图采用了API Gateway + sidecar来作为Ingress，和原始的Istio Ingress有所不同，但流量转发逻辑类似。Istio Ingress和API Gateway的差异分析参见文章：https://zhaohuabing.com/post/2018-12-27-the-obstacles-to-put-istio-into-production/#service-mesh-and-api-gateway 从上图可以看到，客户端请求从进入系统入口的Ingress，到到达后端提供服务的应用，经过了多次IPtable的重定向和Envoy的转发。 其转发流程如下：（备注：为简略起见，本流程中未描述客户端端口） 1: Client IP ---> Ingress Public IP: Server Port 2: Ingress Internal IP ---> Service 1 IP: Server Port 2.1: Ingress Internal IP ----(IPtable DNAT)---> 127.0.0.1: 15001 2.2: Ingress Internal IP---> Service1 IP:Server Port 2.2.1: Ingress Internal IP ----(IPtable DNAT)---> 127.0.0.1: 15001 2.2.2: Service1 Sidecar IP ---> 127.0.0.1: Service1 Server Port 3: Service1 IP ---> Service2 Server Port 3.1: Service1 IP ----(IPtable DNAT)---> 127.0.0.1: 15001 3.2: Service1 IP ----> Service2 Server Port ...... 如果Istio配置错误导致通信故障，从应用层面上很难直接查找原因。需要通过各种手段从TCP通讯层，Pilot,Envoy等多处获取信息，对故障进行分析。 Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 12:40:49 "},"content/debug-istio/Pilot调试信息.html":{"url":"content/debug-istio/Pilot调试信息.html","title":"Pilot调试信息","keywords":"","body":"Pilot提供了一个调试端口9093，可以通过向调试端口发送REST请求来分析和查看标准数据面接口(Envoy xDS API)的数据和Pilot内部存储的状态信息。 xDS接口相关调试信息 发送给Enovy的Listener，Filter及Route配置 curl http://127.0.0.1:9093/debug/adsz 各个Cluster中配置的Endpoint curl http://127.0.0.1:9093/debug/edsz Cluster信息 curl http://127.0.0.1:9093/debug/cdsz 备注：上述接口中的配置信息在Envoy第一次连接到Pilot中时才会生成，在此之前，通过接口无法获取到数据。 Pilot内部的配置信息 服务注册表信息 curl http://127.0.0.1:9093/debug/registryz 所有的endpoint curl http://127.0.0.1:9093/debug/endpointz[?brief=1] 所有的配置信息 curl http://127.0.0.1:9093/debug/configz Pilot自身的一些性能数据 curl http://127.0.0.1:9093/metrics 参考：https://github.com/istio/istio/tree/master/pilot/pkg/proxy/envoy/v2 Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 12:40:49 "},"content/debug-istio/Envoy调试信息.html":{"url":"content/debug-istio/Envoy调试信息.html","title":"Envoy调试信息","keywords":"","body":"查看Envoy配置 Envoy在localhost:15000上提供了Admin端口，可以通过docker exec命令获取其Pilot向其下发的配置信息。 sudo docker exec a83696c9e7a2 curl http://127.0.0.1:15000/config_dump 除此以外，Envoy还提供了其他调试信息，可以通过help进行查询 sudo docker exec a83696c9e7a2 curl http://127.0.0.1:15000/help admin commands are: /: Admin home page /certs: print certs on machine /clusters: upstream cluster status /config_dump: dump current Envoy configs (experimental) /cpuprofiler: enable/disable the CPU profiler /healthcheck/fail: cause the server to fail health checks /healthcheck/ok: cause the server to pass health checks /help: print out list of admin commands /hot_restart_version: print the hot restart compatibility version /listeners: print listener addresses /logging: query/change logging levels /quitquitquit: exit the server /reset_counters: reset all counters to zero /runtime: print runtime values /runtime_modify: modify runtime values /server_info: print server version/status information /stats: print server stats /stats/prometheus: print server stats in prometheus format 参考： https://www.envoyproxy.io/docs/envoy/latest/operations/admin IPtable规则 proxy_init 容器会将设置的IPtable内容输出到标准输出中，可以查看到对那些IP端进行了拦截。 sudo docker logs 3ad9 ......忽略掉前面无关的内容...... # Generated by iptables-save v1.6.0 on Fri Jan 11 07:10:19 2019 *nat :PREROUTING ACCEPT [0:0] :INPUT ACCEPT [0:0] :OUTPUT ACCEPT [0:0] :POSTROUTING ACCEPT [0:0] :ISTIO_OUTPUT - [0:0] :ISTIO_REDIRECT - [0:0] -A PREROUTING -m comment --comment \"istio/install-istio-prerouting\" -j ISTIO_REDIRECT -A OUTPUT -p tcp -m comment --comment \"istio/install-istio-output\" -j ISTIO_OUTPUT -A ISTIO_OUTPUT ! -d 127.0.0.1/32 -o lo -m comment --comment \"istio/redirect-implicit-loopback\" -j ISTIO_REDIRECT -A ISTIO_OUTPUT -m owner --uid-owner 1337 -m comment --comment \"istio/bypass-envoy\" -j RETURN -A ISTIO_OUTPUT -d 127.0.0.1/32 -m comment --comment \"istio/bypass-explicit-loopback\" -j RETURN -A ISTIO_OUTPUT -d 172.168.40.4/32 -m comment --comment \"istio/bypass-msb-ip\" -j RETURN -A ISTIO_OUTPUT -d 100.100.0.0/16 -m comment --comment \"istio/redirect-ip-range-100.100.0.0/16\" -j ISTIO_REDIRECT -A ISTIO_OUTPUT -d 172.168.40.0/24 -m comment --comment \"istio/redirect-ip-range-172.168.40.0/24\" -j ISTIO_REDIRECT -A ISTIO_OUTPUT -m comment --comment \"istio/bypass-default-outbound\" -j RETURN -A ISTIO_REDIRECT -p tcp -m comment --comment \"istio/redirect-to-envoy-port\" -j REDIRECT --to-ports 15001 COMMIT # Completed on Fri Jan 11 07:10:19 2019 Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 12:40:49 "},"content/debug-istio/Consul调试信息.html":{"url":"content/debug-istio/Consul调试信息.html","title":"Consul调试信息","keywords":"","body":"如果采用了Consul作为Service Registry，可以通过下面的接口查看Consul中的服务注册信息，以和Pilot及Envoy中的服务信息进行对比分析。 查看Consul中注册的所有服务 curl http://172.167.40.2:1107/v1/catalog/services 查看某一个服务的具体内容 curl http://172.167.40.2:1107/v1/catalog/service/{service} 参考： https://www.consul.io/api/catalog.html Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 12:40:49 "},"content/debug-istio/协议层调试信息.html":{"url":"content/debug-istio/协议层调试信息.html","title":"协议层调试信息","keywords":"","body":"Strace是一个linux系统下的小工具，可以用于跟踪一个指定进程的系统调用和其接收到的数据。该工具可以查看写入/收到file descriptor的内容，可以被用于分析微服务间的HTTP通信。 首先通过ps找到需要跟踪的进程。 sudo ps -ef|grep istio-demo 然后使用strace来监控该进程的网络消息。 sudo strace -p 91558 -f -e trace=network -s 1000 下面是strace对一个HTTP请求的相关调试输出信息，可以看到该进程对外发出了一个http请求，并收到了一个404错误。可以从输出中查看到HTTP请求的完整内容，包括Method, URL, Header等内容。 [pid 93648] sendto(285, \"GET /api/istioserver/v1/animals/panda HTTP/1.1\\r\\nHost: 100.100.0.112:9090\\r\\nConnection: Keep-Alive\\r\\nAccept-Encoding: gzip\\r\\nUser-Agent: okhttp/3.3.0\\r\\n\\r\\n\", 149, 0, NULL, 0) = 149 [pid 93648] recvfrom(285, \"HTTP/1.1 404 Not Found\\r\\ndate: Fri, 11 Jan 2019 04:59:07 GMT\\r\\nserver: envoy\\r\\ncontent-length: 0\\r\\n\\r\\n\", 8192, MSG_DONTWAI strace的使用方法可参考： http://man7.org/linux/man-pages/man1/strace.1.html Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 12:40:49 "},"content/ingress/nodeport.html":{"url":"content/ingress/nodeport.html","title":"外部通信-NodePort","keywords":"","body":"如何从外部网络访问 Kubernetes的Pod IP和Cluster IP都只能在集群内部访问，而我们通常需要从外部网络上访问集群中的某些服务，Kubernetes提供了下述几种方式来为集群提供外部流量入口。 NodePort NodePort在集群中的主机节点上为Service提供一个代理端口，以允许从主机网络上对Service进行访问。Kubernetes官网文档只介绍了NodePort的功能，并未对其实现原理进行解释。下面我们通过实验来分析NodePort的实现机制。 www.katacoda.com 这个网站提供了一个交互式的Kubernetes playground，注册即可免费实验kubernetes的相关功能，下面我们就使用Katacoda来分析Nodeport的实现原理。 在浏览器中输入这个网址：https://www.katacoda.com/courses/kubernetes/networking-introduction， 打开后会提供了一个实验用的Kubernetes集群，并可以通过网元模拟Terminal连接到集群的Master节点。 执行下面的命令创建一个nodeport类型的service。 kubectl apply -f nodeport.yaml 查看创建的service，可以看到kubernetes创建了一个名为webapp-nodeport-svc的service，并为该service在主机节点上创建了30080这个Nodeport。 master $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 443/TCP 36m webapp1-nodeport-svc NodePort 10.103.188.73 80:30080/TCP 3m webapp-nodeport-svc后端对应两个Pod，其Pod的IP分别为10.32.0.3和10.32.0.5。 master $ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IPNODE NOMINATED NODE webapp1-nodeport-deployment-785989576b-cjc5b 1/1 Running 0 2m 10.32.0.3 webapp1-nodeport-deployment-785989576b-tpfqr 1/1 Running 0 2m 10.32.0.5 通过netstat命令可以看到Kube-proxy在主机网络上创建了30080监听端口，用于接收从主机网络进入的外部流量。 master $ netstat -lnp|grep 30080 tcp6 0 0 :::30080 :::* LISTEN 7427/kube-proxy 下面是Kube-proxy创建的相关iptables规则以及对应的说明。可以看到Kube-proxy为Nodeport创建了相应的IPtable规则，将发向30080这个主机端口上的流量重定向到了后端的两个Pod IP上。 iptables-save > iptables-dump # Generated by iptables-save v1.6.0 on Thu Mar 28 07:33:57 2019 *nat # Nodeport规则链 :KUBE-NODEPORTS - [0:0] # Service规则链 :KUBE-SERVICES - [0:0] # Nodeport和Service共用的规则链 :KUBE-SVC-J2DWGRZTH4C2LPA4 - [0:0] :KUBE-SEP-4CGFRVESQ3AECDE7 - [0:0] :KUBE-SEP-YLXG4RMKAICGY2B3 - [0:0] # 将host上30080端口的外部tcp流量转到KUBE-SVC-J2DWGRZTH4C2LPA4链 -A KUBE-NODEPORTS -p tcp -m comment --comment \"default/webapp1-nodeport-svc:\" -m tcp --dport 30080 -j KUBE-SVC-J2DWGRZTH4C2LPA4 #将发送到Cluster IP 10.103.188.73的内部流量转到KUBE-SVC-J2DWGRZTH4C2LPA4链 KUBE-SERVICES -d 10.103.188.73/32 -p tcp -m comment --comment \"default/webapp1-nodeport-svc: cluster IP\" -m tcp --dport 80 -j KUBE-SVC-J2DWGRZTH4C2LPA4 #将发送到webapp1-nodeport-svc的流量转交到第一个Pod（10.32.0.3）相关的规则链上，比例为50% -A KUBE-SVC-J2DWGRZTH4C2LPA4 -m comment --comment \"default/webapp1-nodeport-svc:\" -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-YLXG4RMKAICGY2B3 #将发送到webapp1-nodeport-svc的流量转交到第二个Pod（10.32.0.5）相关的规则链上 -A KUBE-SVC-J2DWGRZTH4C2LPA4 -m comment --comment \"default/webapp1-nodeport-svc:\" -j KUBE-SEP-4CGFRVESQ3AECDE7 #将请求重定向到Pod 10.32.0.3 -A KUBE-SEP-YLXG4RMKAICGY2B3 -p tcp -m comment --comment \"default/webapp1-nodeport-svc:\" -m tcp -j DNAT --to-destination 10.32.0.3:80 #将请求重定向到Pod 10.32.0.5 -A KUBE-SEP-4CGFRVESQ3AECDE7 -p tcp -m comment --comment \"default/webapp1-nodeport-svc:\" -m tcp -j DNAT --to-destination 10.32.0.5:80 从上面的实验可以看到，通过将一个Service定义为NodePort类型，Kubernetes会通过集群中node上的Kube-proxy为该Service在主机网络上创建一个监听端口。Kube-proxy并不会直接接收该主机端口进入的流量，而是会创建相应的Iptables规则，并通过Iptables将从该端口收到的流量直接转发到后端的Pod中。 NodePort的流量转发机制和Cluster IP的iptables模式类似，唯一不同之处是在主机网络上开了一个“NodePort”来接受外部流量。从上面的规则也可以看出，在创建Nodeport时，Kube-proxy也会同时为Service创建Cluster IP相关的iptables规则。 备注：除采用iptables进行流量转发，NodePort应该也可以提供userspace模式以及ipvs模式，这里未就这两种模式进行实验验证。 从分析得知，在NodePort模式下，集群内外部的通讯如下图所示： Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 13:15:37 "},"content/ingress/loadbalancer.html":{"url":"content/ingress/loadbalancer.html","title":"外部通讯-LoadBalancer","keywords":"","body":"LoadBalancer NodePort提供了一种从外部网络访问Kubernetes集群内部Service的方法，但该方法存在下面一些限制，导致这种方式主要适用于程序开发，不适合用于产品部署。 Kubernetes cluster host的IP必须是一个well-known IP，即客户端必须知道该IP。但Cluster中的host是被作为资源池看待的，可以增加删除，每个host的IP一般也是动态分配的，因此并不能认为host IP对客户端而言是well-known IP。 客户端访问某一个固定的host IP的方式存在单点故障。假如一台host宕机了，kubernetes cluster会把应用 reload到另一节点上，但客户端就无法通过该host的nodeport访问应用了。 通过一个主机节点作为网络入口，在网络流量较大时存在性能瓶颈。 为了解决这些问题，Kubernetes提供了LoadBalancer。通过将Service定义为LoadBalancer类型，Kubernetes在主机节点的NodePort前提供了一个四层的负载均衡器。该四层负载均衡器负责将外部网络流量分发到后面的多个节点的NodePort端口上。 下图展示了Kubernetes如何通过LoadBalancer方式对外提供流量入口，图中LoadBalancer后面接入了两个主机节点上的NodePort，后端部署了三个Pod提供服务。根据集群的规模，可以在LoadBalancer后面可以接入更多的主机节点，以进行负荷分担。 图片 - NodeBalancer 备注：LoadBalancer类型需要云服务提供商的支持，Service中的定义只是在Kubernetes配置文件中提出了一个要求，即为该Service创建Load Balancer，至于如何创建则是由Google Cloud或Amazon Cloud等云服务商提供的，创建的Load Balancer的过程不在Kubernetes Cluster的管理范围中。 目前WS, Azure, CloudStack, GCE 和 OpenStack 等主流的公有云和私有云提供商都可以为Kubernetes提供Load Balancer。一般来说，公有云提供商还会为Load Balancer提供一个External IP，以提供Internet接入。如果你的产品没有使用云提供商，而是自建Kubernetes Cluster，则需要自己提供LoadBalancer。 Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 13:16:00 "},"content/ingress/ingress.html":{"url":"content/ingress/ingress.html","title":"外部通讯-Ingress","keywords":"","body":"Ingress LoadBalancer类型的Service提供的是四层负载均衡器，当只需要向外暴露一个服务的时候，采用这种方式是没有问题的。但当一个应用需要对外提供多个服务时，采用该方式则要求为每一个四层服务（IP+Port）都创建一个外部load balancer。 一般来说，同一个应用的多个服务/资源会放在同一个域名下，在这种情况下，创建多个Load balancer是完全没有必要的，反而带来了额外的开销和管理成本。另外直接将服务暴露给外部用户也会导致了前端和后端的耦合，影响了后端架构的灵活性，如果以后由于业务需求对服务进行调整会直接影响到客户端。为了解决该问题，可以通过使用Kubernetes Ingress来作为网络入口。 Ingress 功能介绍 Kubernetes Ingress声明了一个应用层（OSI七层）的负载均衡器，可以根据HTTP请求的内容将来自同一个TCP端口的请求分发到不同的Kubernetes Service，其功能包括： 按HTTP请求的URL进行路由 同一个TCP端口进来的流量可以根据URL路由到Cluster中的不同服务，如下图所示： 图片 - Simple fanout 按HTTP请求的Host进行路由 同一个IP进来的流量可以根据HTTP请求的Host路由到Cluster中的不同服务，如下图所示： 图片 - Name based virtual hosting Ingress 规则定义了对七层网关的要求，包括URL分发规则，基于不同域名的虚拟主机，SSL证书等。Kubernetes使用Ingress Controller 来监控Ingress规则，并通过一个七层网关来实现这些要求，一般可以使用Nginx，HAProxy，Envoy等。 Ingress配合NodePort和LoadBalancer提供对外流量入口 虽然Ingress Controller通过七层网关为后端的多个Service提供了统一的入口，但由于其部署在集群中，因此并不能直接对外提供服务。实际上Ingress需要配合NodePort和LoadBalancer才能提供对外的流量入口，如下图所示： 图片 - 采用Ingress, NodePortal和LoadBalancer提供外部流量入口的拓扑结构 上图描述了如何采用Ingress配合NodePort和Load Balancer为集群提供外部流量入口，从该拓扑图中可以看到该架构的伸缩性非常好，在NodePort，Ingress，Pod等不同的接入层面都可以对系统进行水平扩展，以应对不同的外部流量要求。 上图只展示了逻辑架构，下面的图展示了具体的实现原理： 图片 - 采用Ingress, NodePortal和LoadBalancer提供外部流量入口的实现原理 流量从外部网络到达Pod的完整路径如下： 外部请求先通过四层Load Balancer进入内部网络 Load Balancer将流量分发到后端多个主机节点上的NodePort (userspace转发) 请求从NodePort进入到Ingress Controller (iptabes规则，Ingress Controller本身是一个NodePort类型的Service) Ingress Controller根据Ingress rule进行七层分发，根据HTTP的URL和Host将请求分发给不同的Service (userspace转发) Service将请求最终导入到后端提供服务的Pod中 (iptabes规则) 从前面的介绍可以看到，K8S Ingress提供了一个基础的七层网关功能的抽象定义，其作用是对外提供一个七层服务的统一入口，并根据URL/HOST将请求路由到集群内部不同的服务上。 Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 13:20:13 "},"content/ingress/k8s-ingress-as-mesh-gateway.html":{"url":"content/ingress/k8s-ingress-as-mesh-gateway.html","title":"采用K8s Ingress作为网格的流量入口","keywords":"","body":"如何为服务网格选择入口网关？ 在Istio服务网格中，通过为每个Service部署一个sidecar代理，Istio接管了Service之间的请求流量。控制面可以对网格中的所有sidecar代理进行统一配置，实现了对网格内部流量的路由控制，从而可以实现灰度发布，流量镜像，故障注入等服务管控功能。但是，Istio并没有为入口网关提供一个较为完善的解决方案。 K8s Ingress 在0.8版本以前，Istio缺省采用K8s Ingress来作为Service Mesh的流量入口。K8s Ingress统一了应用的流量入口，但存在两个问题： K8s Ingress是独立在Istio体系之外的，需要单独采用Ingress rule进行配置，导致系统入口和内部存在两套互相独立的路由规则配置，运维和管理较为复杂。 K8s Ingress rule的功能较弱，不能在入口处实现和网格内部类似的路由规则，也不具备网格sidecar的其它能力，导致难以从整体上为应用系统实现灰度发布、分布式跟踪等服务管控功能。 Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 13:28:03 "},"content/ingress/istio-gateway-as-mesh-gateway.html":{"url":"content/ingress/istio-gateway-as-mesh-gateway.html","title":"采用Istio Gateway作为网络的流量入口","keywords":"","body":"Istio Gateway Istio社区意识到了Ingress和Mesh内部配置割裂的问题，因此从0.8版本开始，社区采用了 Gateway 资源代替K8s Ingress来表示流量入口。 Istio Gateway资源本身只能配置L4-L6的功能，例如暴露的端口，TLS设置等；但Gateway可以和绑定一个VirtualService，在VirtualService 中可以配置七层路由规则，这些七层路由规则包括根据按照服务版本对请求进行导流，故障注入，HTTP重定向，HTTP重写等所有Mesh内部支持的路由规则。 Gateway和VirtualService用于表示Istio Ingress的配置模型，Istio Ingress的缺省实现则采用了和Sidecar相同的Envoy proxy。 通过该方式，Istio控制面用一致的配置模型同时控制了入口网关和内部的sidecar代理。这些配置包括路由规则，策略检查、Telementry收集以及其他服务管控功能。 Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 13:28:25 "},"content/ingress/api-gateway-requirement.html":{"url":"content/ingress/api-gateway-requirement.html","title":"服务化应用对API Gateway的功能需求","keywords":"","body":"应用对API Gateway的需求 采用Gateway和VirtualService实现的Istio Ingress Gateway提供了网络入口处的基础通信功能，包括可靠的通信和灵活的路由规则。但对于一个服务化应用来说，网络入口除了基础的通讯功能之外，还有一些其他的应用层功能需求，例如： 第三方系统对API的访问控制 用户对系统的访问控制 修改请求/返回数据 服务API的生命周期管理 服务访问的SLA、限流及计费 …. 图片 - Kubernetes ingress, Istio gateway and API gateway的功能对比 API Gateway需求中很大一部分需要根据不同的应用系统进行定制，目前看来暂时不大可能被纳入K8s Ingress或者Istio Gateway的规范之中。为了满足这些需求，涌现出了各类不同的k8s Ingress Controller以及Istio Ingress Gateway实现，包括Ambassador ，Kong, Traefik,Solo等。 这些网关产品在实现在提供基础的K8s Ingress能力的同时，提供了强大的API Gateway功能，但由于缺少统一的标准，这些扩展实现之间相互之间并不兼容。而且遗憾的是，目前这些Ingress controller都还没有正式提供和Istio 控制面集成的能力。 备注： Ambassador将对Istio路由规则的支持纳入了Roadmap https://www.getambassador.io/user-guide/with-istio/ Istio声称支持Istio-Based Route Rule Discovery (尚处于实验阶段) https://gloo.solo.io/introduction/architecture/ Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 13:28:49 "},"content/ingress/mesh-gateway-solution.html":{"url":"content/ingress/mesh-gateway-solution.html","title":"服务网格入口网关的解决方案","keywords":"","body":"采用API Gateway + Sidecar Proxy作为服务网格的流量入口 在目前难以找到一个同时具备API Gateway和Isito Ingress能力的网关的情况下，一个可行的方案是使用API Gateway和Sidecar Proxy一起为服务网格提供外部流量入口。 由于API Gateway已经具备七层网关的功能，Mesh Ingress中的Sidecar只需要提供VirtualService资源的路由能力，并不需要提供Gateway资源的网关能力，因此采用Sidecar Proxy即可。网络入口处的Sidecar Proxy和网格内部应用Pod中Sidecar Proxy的唯一一点区别是：该Sidecar只接管API Gateway向Mesh内部的流量，并不接管外部流向API Gateway的流量；而应用Pod中的Sidecar需要接管进入应用的所有流量。 图片 - 采用API Gateway + Sidecar Proxy为服务网格提供流量入口 备注：在实际部署时，API Gateway前端需要采用NodePort和LoadBalancer提供外部流量入口。为了突出主题，对上图进行了简化，没有画出NodePort和LoadBalancer。 采用API Gateway和Sidecar Proxy一起作为服务网格的流量入口，既能够通过对网关进行定制开发满足产品对API网关的各种需求，又可以在网络入口处利用服务网格提供的灵活的路由能力和分布式跟踪，策略等管控功能，是服务网格产品入口网关的一个理想方案。 性能方面的考虑：从上图可以看到，采用该方案后，外部请求的处理流程在入口处增加了Sidecar Proxy这一跳，因此该方式会带来少量的性能损失，但该损失是完全可以接受的。 对于请求时延而言，在服务网格中，一个外部请求本来就要经过较多的代理和应用进程的处理，在Ingress处增加一个代理对整体的时延影响基本忽略不计，而且对于绝大多数应用来说，网络转发所占的时间比例本来就很小，99%的耗时都在业务逻辑。如果系统对于增加的该时延非常敏感，则建议重新考虑该系统是否需要采用微服务架构和服务网格。 对于吞吐量而言，如果入口处的网络吞吐量存在瓶颈，则可以通过对API Gateway + Sidecar Proxy组成的Ingress整体进行水平扩展，来对入口流量进行负荷分担，以提高网格入口的网络吞吐量。 参考 本章节参考了下述链接中的文档。 Virtual IPs and Service Proxie - kubernetes.io 如何从外部访问Kubernetes集群中的应用？ - zhaohuabing.com The obstacles to put Istio into production and how we solve them - kubernetes.io Kubernetes NodePort vs LoadBalancer vs Ingress? When should I use what? - medium.com Copyright © zhaohuabing.com 2019 all right reserved，powered by Gitbook Updated at 2019-03-30 13:29:57 "}}